{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will see the [DQN algorithm](https://en.wikipedia.org/wiki/Q-learning) in action learning to collect yellow bananas inside a Unity ML-Agents environment.\n",
    "\n",
    "This was part of the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) done in October 2018. \n",
    "\n",
    "## 1. Collecting Bananas?\n",
    "\n",
    "Yep! I'm originally from Colombia and we like Bananas so much that this tasks sounds pretty (yummy) interesting. Going back to the serious stuff, this environment is a small step towards the holly-grail autonomous navigation. The agent needs to learn to move by itself inside an environment reinforce by the fact that he is eating delicious bananas.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/10624937/42135619-d90f2f28-7d12-11e8-8823-82b970a54d7e.gif\" /> \n",
    "\n",
    "In the previous video, we could watch an informative and straightforward environment to assess free navigation of agents. The challenge consists in autonomously devise a plan to collect as many yellow bananas (healthy) while avoid collecting the blue bananas (rotten). Note that the bananas are randomly placed such that the agent does not learn to memorize locations.\n",
    "\n",
    "Before moving forward let's double check that you have all the necessary packages. If the code cell below returns an error, please revisit the instructions in the README to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md), [NumPy](http://www.numpy.org/) and [Pytorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "from dqn_agent import Agent\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a Environment setup\n",
    "\n",
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "file_name = \"Banana.app\"\n",
    "env = UnityEnvironment(file_name=file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unity environments contain *brains* which are responsible for deciding the actions of their associated agents.\n",
    "\n",
    "As the simulation contains a single agent that navigates a large environment. In this case, at each time step our a has four actions at its disposal:\n",
    "\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana (healthy), and a reward of `-1` is provided for collecting a blue banana (rotten). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning with DQN\n",
    "\n",
    "Based on the previous details of the environment, we are ready to discuss how to (automatically) devise a strategy to collect as many yellows bananas as possible. In this case, we will use the Q-learning algorithm.\n",
    "\n",
    "Q-learning is a generic algorithm that distills a particular strategy for solving a problem without explicitly coding up the actions of the agent at every step. How does it do that?\n",
    "\n",
    "### 2.1 The setup\n",
    "\n",
    "As you will find out, it does that in a very simple way. Q-learning relies on the fact that we can freely interact and receive rewards with/from the environment which is precisely the case outlined in the previous section. Every moment in time, our agent collects information about the environment, i.e. itself and the things in front of it (the so-called state space of `37` dimensions); and it takes action (from 0-4 which form the so-called action space). Subsequently, it will recive another packet of information, i.e. a new `37` dimensional vector; and the reward (`+1/-1` for collecting a yellow/blue banana or `0` othersiwe, i.e. nothing was collected). This process will repeat for a period of time, and our agent will be able to assess how many bananas it collects following a particular strategy.\n",
    "\n",
    "Pythonically speaking we could summarize this process in the following way:\n",
    "\n",
    "```python\n",
    "def interaction(agent, env, eps=1):\n",
    "    \"Return the experience of a single interaction\"\n",
    "    brain_name = env.brain_names[0]\n",
    "    state = env[brain_name].vector_observations[0]\n",
    "    \n",
    "    action = agent.act(state, eps)\n",
    "    \n",
    "    env_info = env.step(action)[brain_name]\n",
    "    next_state = env[brain_name].vector_observations[0]\n",
    "    reward = env[brain_name].rewards[0]\n",
    "    done = env[brain_name].local_done[0]\n",
    "    return state, action, next_state, reward, done\n",
    "```\n",
    "\n",
    "This function grants our agent with access to an environment where it can safely play and collect a reward. To unroll multiple instances of this, we only place the previous function inside a `while` loop which runs over a finite horizon determined by the environment (`done == True`).\n",
    "\n",
    "```python\n",
    "score = 0\n",
    "while True:\n",
    "    # Interaction\n",
    "    state, action, next_state, reward, done = interaction(\n",
    "        agent, env)\n",
    "    score += reward\n",
    "\n",
    "    # Collecting or Learning\n",
    "    agent.step(state, action, reward, next_state, done)\n",
    "    if done:\n",
    "        break\n",
    "```\n",
    "\n",
    "Note that after we interact with the environment, our agent will proceed to \"collect or learn\" from this experience. We will discuss this step in more details in what follows.\n",
    "\n",
    "### 2.1 Q-learning core\n",
    "\n",
    "In layman terms, we can think about Q-learning as an update rule of a guess with another guess.\n",
    "\n",
    "Let's add a bit of notation to not get loss with hand wavy explanations. Let's pretend  that our agents somehow is able to map a given environment state $s \\in \\mathbb{R}^{37}$ into an action $a \\in \\{0,1,2,3\\}$. More contretely, let's assume that our agent is capable to determine the value of each of its actions $A$ for a given state $s \\in S$ which we would denote as $Q(s, a; w)$.\n",
    "\n",
    "> $w$ represents the internal parameters of our agent and the denotation $Q(\\dots;w)$ only strengh the fact that our function $Q$ is tied to $w$.\n",
    "\n",
    "The Q-learning algorithm proposes to update the $Q(\\dots;w)$ function of our agent by only focusing on the current <b>S</b>tate $s$, <b>A</b>ction $a$, subsequent <b>R</b>eward $r$, the next <b>S</b>tate $s'$, the most advantageous next <b>A</b>ction as follows:\n",
    "\n",
    "$\\Delta w = \\alpha \\color{red}{(} r + \\gamma max_a Q(s',a; w) - Q(s, a; w)\\color{red}{)} \\nabla_{w} Q(s, a; w)$\n",
    "\n",
    "where $\\alpha$ is the so-called learning rate which determines how fast we update; and $\\gamma$ is a discount factor to give more or less priority to the inmediate reward.\n",
    "\n",
    "For simplicity, let's disregard the gradient operation $\\nabla_{w} Q(s, a; w)$ and the factor $\\alpha$ as they only accounts for the fact that the $Q(\\dots;w)$ function of our agent is differentiable and the rate of update of the parameters $w$.\n",
    "\n",
    "The term inside the $\\color{red}{\\text{red}}$ parentheses represents the error of our agent on the estimation of the expected action-state value function $Q(s, a; w)$ which is usually called [_TD-Error_](https://en.wikipedia.org/wiki/Temporal_difference_learning). In a nutshell, all what the update is telling us is to change $w$ such that the $Q(s, a;w)$ function of our agents predicts values closer to the expected return i.e. the reward $r$ and the optimal future reward $\\gamma max_a Q(s',a; w)$ in the next state $s'$.\n",
    "\n",
    "It would be overly optimistic to assume that our agent will learn to navigate from a single episode or trial with the environment. Thus, it's fair to repeat the same procedure multiple times or `episodes` as follow:\n",
    "\n",
    "```python\n",
    "scores = []\n",
    "for i_episode in range(1, n_episodes + 1):\n",
    "    env_info = env.reset(train_mode=True)[env.brain_names[0]]\n",
    "    score = 0\n",
    "    while True:\n",
    "        # Interaction\n",
    "        state, action, next_state, reward, done = interaction(\n",
    "            agent, env, eps)\n",
    "        score += reward\n",
    "\n",
    "         # Learn - Collect\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "        if done:\n",
    "            break                \n",
    "    \n",
    "    scores.append(score)\n",
    "\n",
    "    # epsilon greedy policy\n",
    "    eps = max(eps_end, eps_decay * eps)\n",
    "```\n",
    "\n",
    "The last line inside the `for` loop only means that we make use of an [epsilon greedy policy](https://jamesmccaffrey.wordpress.com/2017/11/30/the-epsilon-greedy-algorithm/) to determine the action of our agent. Roughly speaking, it allows that our agent takes random actions from time to time instead of blindly focusing on its $Q(s, a;w)$. Think about the first round of episodes where our agent strategy may not be optimal, just trusting that would yield into a sub-optimal strategy. In this particular case, we are using an exponential decay of the epsilon values as the episodes progress. Notice as well that we clamp its value such that it's at least `0.1`. The mathematical form of `\\epsilon` in terms of the episodes is given by the following equation $\\epsilon_t = \\max ( \\rho^t \\epsilon_0, \\epsilon_{\\text{min}})$. In the code snippet $\\rho$ is called `eps_decay`, $\\epsilon_t$ is called `eps`, and $\\epsilon_{\\text{min}}$ is `eps_end`. In practice, we found that a `eps_decay=0.95` and `eps_end=0.1` produces nice result in this environment.\n",
    "\n",
    "### 2.3 So shall we update $w$ at every time step?\n",
    "\n",
    "In principle, the Q-learning algorithm is concerned about how to update the parameters $w$ of the $Q$ function of our agent. Q-learning does not necessarily force our agent to do it at every step only focusing on the last batch of $(s, t, r, s')$. Indeed, blindly updating $w$ at every time step could cause troubles due to harmful correlations introduces by the inherent sequential nature in which our agents interacts with the environment.\n",
    "\n",
    "Two possible enhancements to avoid them are described below:\n",
    "\n",
    "- Fixed Q-network. Let's focus on the TD-error, the term inside $\\color{red}{\\text{red}}$ parentheses, of the update rule in the Q-learning algorithm.\n",
    "\n",
    "  $\\Delta w = \\alpha \\color{red}{(} r + \\gamma max_a Q(s',a; w) - Q(s, a; w)\\color{red}{)} \\nabla_{w} Q(s, a; w)$\n",
    "  \n",
    "  As we mentioned before, Q-learning update a guess ($Q(s, a;w$) with another guess ($Q(s', a;w$). Given that the guess of our agent could be noisy, as she is learning, this could introduce harmful correlations.\n",
    "  \n",
    "  So what can we do?\n",
    "  \n",
    "  One simple solution it's to fixed one of these two moving pieces, and make our agent to improve upon an \"old\" version of itself. In math terms, let's ammend the TD-error as follows:\n",
    "  \n",
    "  $\\color{red}{(} r + \\gamma max_a Q(s',a; w^{-}) - Q(s, a; w)\\color{red}{)}$\n",
    "  \n",
    "  here $w^{-}$ are parameters of an old version of the $Q$ function of our agent. As our agent collects more episodes, it updates $w^{-}$ but a slower pace such as $w^- = \\tau*w + (1 - \\tau)*w^-$. Here $\\tau \\in [0, 1]$ determine how similar ($\\tau \\rightarrow 1$) would become our old version $w^-$ to our current updated weights $w$.\n",
    "  \n",
    "  The implementation of this particular step corresponds to the method `soft_update` inside our class `Agent`. In practice, $\\tau = 1e-3$ yields satisfying results for our agent.\n",
    "\n",
    "- Experience replay: Think about the way that we learn in the real world, we apply some level of self-assessment or introspection about our actions and the experiences that we collect in the past. Wouldn't it be wasteful if our agent updates merely the parameters of its $Q$ function and move onto the next experience forgetting what it just observed? It sounds that it is.\n",
    "\n",
    "  That's one of the motivations behind experience replay. Rather than applying the Q-learning update blindly at each time step, it collects multiple experiences in a tape and performs updates from time to time $Q(\\dots;w)$ by sampling uniformly at random a handful set of experiences from the tape.\n",
    "  \n",
    "  The nice thing about experience replay is that is really simple to implement and easily breaks harmful correlations introduced by implementing any sequential scheme of the Q-learning update rule. Think about it, let's assume that our agent has interacted with the environment for many episodes, and that is buffer is big enough. No matter if she is in a new episode, information from previous episodes could come into its Q-learning update in the current episode. In this way, the agent can re-use information across episodes instead on blindly updating its $Q$ function with the latest set of experiences.\n",
    "  \n",
    "  The implementation details of the experience replay can be found in the class `ReplayBuffer`. There, you will find that the replay buffer is mainly composed by an `add` method, which always appends into a finite length buffer (`deque` data-structure Python); and a `sample` method, which returns elements from the deck sampled uniformly at random.\n",
    "  \n",
    "  Finally, this step gives more sense to the word \"Collecting\" of the comment on top of the first code snippet which invokes this stage.\n",
    "  \n",
    "  ```python\n",
    "  # Collecting or Learning\n",
    "  agent.step(state, action, reward, next_state, done)\n",
    "  ```\n",
    "  \n",
    "  Thus, at every time step our agent takes an \"`step`\" which means (i) simply collecting experience by placing them in its `ReplayBuffer`; or (ii) learning by invoking the `learn` method (Q-learning) as well as doing (i).\n",
    "\n",
    "### 2.4 Implementation details of Q-learning step\n",
    "\n",
    "In case, you are eager to know how to implement this in pytorch. The skeleton of the q-learning steps is provided below. As we mention above `learn` is a method of our class `Agent`, thus it contains a `self` argument. Additionally, you will find a couple of additional attributes such as `self.qnetwork_fixed` which represents the \"old\" version of the $Q(\\dots;w^-)$ function mentioned above. Similarly, `self.qnetwork` corresponds to the current $Q(\\dots,w)$ function of our agent updated in this function.\n",
    "\n",
    "```python\n",
    "def learn(self, experiences, gamma):\n",
    "    \"Update Q value parameters (w) using given batch of experience tuples\"\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    # max_a Q(s', a; w^-)\n",
    "    target_action_values = self.qnetwork_fixed(next_states)\n",
    "    best_action_values = target_action_values.max(dim=1, keepdim=True)[0]\n",
    "    # implementation details: ensure we don't backprop through qnetwork_fixed\n",
    "    best_action_values.detach_()\n",
    "    \n",
    "    # Compute TD-target\n",
    "    # r + \\gamma * max_a Q(s', a; w^-)\n",
    "    expected_state_action_values = (\n",
    "        rewards + gamma * best_action_values * (1 - dones))\n",
    "\n",
    "    # Q(s, a; w)\n",
    "    pred_state_action_values = self.qnetwork(states).gather(1, actions)\n",
    "\n",
    "    # Average( TD-Error^2 )\n",
    "    loss = torch.sum(\n",
    "        (expected_state_action_values - pred_state_action_values).pow(2))\n",
    "\n",
    "    # Optimize the qnetwork\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "```\n",
    "\n",
    "The only difference between the code above and the description from the previous sections is the absence of the explict update rule: $\\Delta w = \\alpha \\color{red}{(} r + \\gamma max_a Q(s',a; w) - Q(s, a; w)\\color{red}{)} \\nabla_{w} Q(s, a; w)$\n",
    "\n",
    "_So isn't there an update of $w$?_\n",
    "\n",
    "there is, but it's done in the last line `self.optimizer.step()`.\n",
    "\n",
    "_Why?_\n",
    "\n",
    "First, note that the update rule contains the gradient with respect to the parameters $w$ of the $Q$ function of our agent. Shortly, pytorch auto-differentiation algorithm makes thing easy by computing that term for us (without explicitly writing down up the gradients). The line `loss.backward()` tells pytorch to compute the gradients. After that, applying the update is simpler 😀. Under the hood, we are using another flavor of [(stochastic) gradient descend](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) called [Adam](https://arxiv.org/abs/1412.6980).\n",
    "\n",
    "### 2.4 Why is the section called DQN?\n",
    "\n",
    "The D and the N comes from the fact that the brain of our agent is a __D__eep [artificial neural **N**etwork](https://en.wikipedia.org/wiki/Artificial_neural_network). Its role is to map the information captured from the environment into an action that would increase the chance that our agent collects as many bananas as possible.\n",
    "\n",
    "In this particular implementation, the architecture of our neural network is a multi-layered perceptron with a single hidden layer of 64 units. Based on the [recent advances](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) in deep learning, we use ReLU ([rectifier linear unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))) as the activation function.\n",
    "\n",
    "Please note that the implementation provided here gives you the flexibility of playing around with the number of hidden units and their size.\n",
    "\n",
    "### 2.1 Putting everything together\n",
    "\n",
    "The following cell put all the snippets described above together. Feel free to play with a couple of the hyper-parameters exposed on the top of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.92\n",
      "Episode 200\tAverage Score: 4.19\n",
      "Episode 300\tAverage Score: 7.66\n",
      "Episode 400\tAverage Score: 10.68\n",
      "Episode 476\tAverage Score: 13.03\n",
      "Environment solved in 376 episodes!\tAverage Score: 13.03\n"
     ]
    }
   ],
   "source": [
    "hidden_units = 64\n",
    "num_layers = 2\n",
    "seed = 1701\n",
    "max_episodes = 1800\n",
    "score_target = 13\n",
    "\n",
    "def state_action_space(env):\n",
    "    \"\"\"Return state and action size\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        env (UnityEnvironment): Unity environment.\n",
    "    \"\"\"\n",
    "    brain_name = env.brain_names[0]\n",
    "    env.reset(train_mode=True)[brain_name]\n",
    "    action_size = env.brains[brain_name].vector_action_space_size\n",
    "    state_size = env.brains[brain_name].vector_observation_space_size\n",
    "    return state_size, action_size\n",
    "\n",
    "def interaction(agent, env, state, eps):\n",
    "    \"\"\"Return the experience of a single interaction\n",
    "        \n",
    "    In a nutshell, the agent takes a single action in the environment\n",
    "    an we record all the observations, the action and the reward.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        agent (Agent): our autonomous agent.\n",
    "        env (UnityEnvironment): Unity environment.\n",
    "        state (numpy array): state representation.\n",
    "    \"\"\"\n",
    "    action = agent.act(state, eps)\n",
    "\n",
    "    brain_name = env.brain_names[0]\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    reward = env_info.rewards[0]\n",
    "    done = env_info.local_done[0]\n",
    "    return action, next_state, reward, done\n",
    "\n",
    "def dqn(agent, env, n_episodes=2000, score_target=13,\n",
    "        eps_start=1.0, eps_end=0.01, eps_decay=0.995,\n",
    "        window_length=100):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        agent (Agent): our autonomous agent.\n",
    "        env (UnityEnvironment): Unity environment.\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=window_length)\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        env_info = env.reset(train_mode=True)[env.brain_names[0]]\n",
    "        state = env_info.vector_observations[0]\n",
    "        \n",
    "        score = 0\n",
    "        while True:\n",
    "            # Interaction\n",
    "            action, next_state, reward, done = interaction(\n",
    "                agent, env, state, eps)\n",
    "            score += reward\n",
    "\n",
    "            # Learn - Collect\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break                \n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        \n",
    "        # epsilon greedy policy\n",
    "        eps = max(eps_end, eps_decay * eps)\n",
    "        \n",
    "        print(f'\\rEpisode {i_episode}\\tAverage Score: '\n",
    "              f'{np.mean(scores_window):.2f}', end=\"\")\n",
    "        if i_episode % window_length == 0:\n",
    "            print(f'\\rEpisode {i_episode}'\n",
    "                  f'\\tAverage Score: {np.mean(scores_window):.2f}')\n",
    "        if np.mean(scores_window) >= score_target:\n",
    "            print(f'\\nEnvironment solved in {i_episode - window_length} episodes!'\n",
    "                  f'\\tAverage Score: {np.mean(scores_window):.2f}')\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "\n",
    "state_size, action_size = state_action_space(env)\n",
    "agent = Agent(state_size=state_size, action_size=action_size,\n",
    "              seed=seed, hidden_units=hidden_units,\n",
    "              num_layers=num_layers)\n",
    "# actual learning!\n",
    "scores = dqn(agent, env, n_episodes=max_episodes,\n",
    "             score_target=score_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you didn't have to wait for too long. In my case, the agent reaches the target of collecting +13 bananas after 376 episodes. Let's take a look at the scores of our agent across all the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores,\n",
    "         label='Agent score')\n",
    "plt.plot([0, len(scores)], [score_target, score_target],\n",
    "         color='red', label='Target score')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tipically, the previous plot is quite noisy due to the stochastic nature of the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Agent in action\n",
    "\n",
    "The plot looks nice, but did our agent learn to collect the bananas?\n",
    "\n",
    "Let's see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 64\n",
    "num_layers = 2\n",
    "snapshot_file = './checkpoint.pth'\n",
    "\n",
    "# query environment for info\n",
    "env_info = env.reset(train_mode=False)[env.brain_names[0]]\n",
    "action_size = env.brains[env.brain_names[0]].vector_action_space_size\n",
    "state_size = len(env_info.vector_observations[0])\n",
    "\n",
    "# setup agent\n",
    "agent = Agent(state_size=state_size, action_size=action_size,\n",
    "              seed=0, hidden_units=hidden_units,\n",
    "              num_layers=num_layers)\n",
    "agent.qnetwork_local.load_state_dict(torch.load(snapshot_file))\n",
    "\n",
    "state = env_info.vector_observations[0]\n",
    "score = 0\n",
    "while True:\n",
    "    action = agent.act(state)\n",
    "    env_info = env.step(action)[env.brain_names[0]]\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    reward = env_info.rewards[0]\n",
    "    done = env_info.local_done[0]\n",
    "    score += reward\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "print('The agent achieves:', score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Departing words\n",
    "\n",
    "It was great to see how a simple algorithm such as DQN managed to solved this particular navigation challenge. There are definitely more steps towards the holy-grail of autonomous navigation. For example,\n",
    "\n",
    "1. More efificient learning.\n",
    "\n",
    "  DQN is great. However, there are a couple of enhacements that could be improved:\n",
    "  -  Prioritized experience replay. Currently, the past experiences used to train our brain is by sampling uniform random. However, we could argue that some experiences may be more \"important\" than others. Moreover, it is possible that such \"important\" experiences are infrequent and our agents ends up sampling them less often. Therefore, it would be ideal to implement a mechanism to sampling experiences in a non-uniform way based on some criterion.\n",
    "  \n",
    "  - Dueling DQN. The current implentation only predicts the \"Q-Value\" function $Q(s, a)$ which is determined by the state and action pair. However, we could decompose such function in terms of a \"State value function\" $V(s)$ and an \"Advantage value function\" $A(s, a)$ such that $Q(s, a) = V(s) + Q(s, a)$. This decomposition has the nice properties of associating a value to the states which inherently promotes a smoother approximation.\n",
    "\n",
    "2. State representation.\n",
    "\n",
    "  Currently, our agent relies on a state representation of `37` diminensions of the environment which seems pretty advance and a bit unnatural. To push forward the limits of autonomous navigation, it is also important to study how to learn such compact representation simply from the raw visual information provided by the simulator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
